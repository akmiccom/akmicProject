# 回帰分析

---

[TOC]

## 回帰分析の目的

- 二つの定量的な関係の構造(モデル)を求めること
- 2変数 $X,Y$ のデータがあるとき、**回帰方程式**と呼ばれる説明の関係を定量的に表す式を求めることが目的

## 説明変数と被説明変数

- 2変数 $X, Y$について説明される変数を$Y$、説明する変数を$X$で表す
    - $Y$ : 従属変数、被説明変数
    - $X$ : 独立変数、説明変数

- $Y$を$X$で説明するのみで逆は成立しないため、相関分析とは本質的に異なる

- $y$ の $x$ 上への回帰方程式(regresion equation)は以下のように表す
  
$$ y = \beta_1 + \beta_2 x $$

- ここでは線形回帰についてのみ考える

## 母回帰方程式

$$ Y_i = \beta_1 + \beta_2 X_i + \varepsilon \ (i = 1, 2, \cdot, n) $$

- $ \beta_1, \beta_2 $ を母(偏)回帰係数といい、母集団の値であるから一般には不明
- これについて推定、検定するのが回帰分析となる
- ただし、$X_i$ は確率変数ではなく、すでに確定した値をとっている

## 誤差項

- $ \varepsilon_i $ は誤差項といい次の3つの条件を満たす確率変数
  - 期待値は $0$ : $ E(\varepsilon_i) = 0, \ i = 1, 2, \cdot, n $
  - 分散は一定で : $\sigma^2$ : $ V(\varepsilon_i) = \sigma^2, \ i = 1, 2, \cdot, n $
  - 異なった誤差項は無相関 : $i\neq j$ ならば $Cov(\varepsilon_i, \varepsilon_j) = E(\varepsilon_i \varepsilon_j) = 0 $
    - このことは $ E(Y_i) = \beta_1 + \beta_2 X_i \ (i = 1, 2, \cdot, n) $ であることを意味している

---